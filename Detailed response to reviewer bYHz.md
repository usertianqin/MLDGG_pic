Thank you very much for taking the time to review our paper. Your insights are invaluable in enhancing the quality and clarity of our research. We have carefully considered each of the points you raisedï¼š

**[W1]:** It is feasible to learn $ğ´$. In practice, to reduce the time and space cost, following the simplification of [36], we convert the sampling of $ğ´^*$ in the structure learner to the product of the (ğ‘ğ‘ƒ)-dimensional matrix $ğµ$ and its transpose. So the complexity is reduced to $ğ‘\timesğ‘ƒ$ but the performance is the same, where $ğ‘ƒ$ is the number of pivots nodes, where $P\ll N$. The complexity reduction details are as follows:

1. Randomly choose $ğ‘ƒ$ nodes in the graph as pivots, where $ğ‘ƒ$ is a hyperparameter.

2. The original $ğ‘\timesğ‘$ adjacency matrix ğ´âˆ—could be retrieved by: $ğ´^*=ğµğµ^T$.

3. The intermediate graph matrix (each element represents the similarity of the corresponding pair of nodes) corresponding to matrix $B$ is defined as $ğ‘‡$. Then the message-passing process when the GNN is applied on $ğ´^*$â€‹can be converted to:
   $$Z^{l+\frac{1}{2}}=RowNorm(T^T)r^{(l)}$$

   $$Z^{l+1}=RowNorm(T)r^{(l+\frac{1}{2})}$$

where $Z^{l+\frac{1}{2}}$ is an intermediate node representation and ğ‘…ğ‘œğ‘¤ğ‘ğ‘œğ‘Ÿğ‘š normalizes the values of each row. Then Eq.4 can be approximated to:
$$\mathcal{B} =-\alpha \sum _ {i.j} {E}^* _ {i,j} || \mathbf{r} _ {i}-\mathbf{r} _ {j} || _ 2^2 - \beta ||{E}^*|| _ {0}$$
where $ğ¸=ğµ^Tğµ$ is the $ğ‘ƒ\times ğ‘ƒ$ pivot-pivot adjacency matrix. Such a procedure can be efficiently conducted within $ğ‘‚(ğ‘ğ‘ƒ)$ time and space complexity. We will emphasize this in the updated version.

**[W2]:** Thank you for your valuable feedback. Firstly, the theory considers the upper and lower bounds of the model generalization to the target domain with distribution shifts in topology structure (A) and attribute features (X). However [22] mainly focus on Euclidean data (e.g. image) and only considers the distribution shift on attribute features. Secondly, the primary emphasis of [22] lies in fairness and precision, topics that are unrelated to our work. Thirdly, all source domains discussed in [22] are generated through StarGAN or CycleGAN, which significantly differs from the source domain settings in our paper. Although the format may appear similar, the context and setup are entirely distinct.
