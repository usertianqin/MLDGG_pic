Thanks for your feedback.

**[W1]:** Thank you for your valuable feedback, we will discuss it in the updated version. The goal of GraphGlow is to learn a generalized structure learner to implement an optimized representation of GNN. Despite GNNs' ability to extract abstract representations, they mix the domain-invariant semantic factor with the domain-specific variation factor. Our framework consisting of a structure learner and a representation learner in the context of meta-learning facilitates knowledge transfer from source and target graphs. The structure learner aims to mitigate the adverse effects of task-unrelated edges, enhancing the comprehensiveness of representations learned by GNNs. Furthermore, the representation learner is to disentangle semantic and variation factors in each domain.

**[W2]:** In our experiments followed by [42], the complexity of the structure learner is $ğ‘‚(ğ‘ğ‘ƒ)$, where $ğ‘ƒ$ is the number of pivot nodes. The complexity of GCNs is $O(|E|Dd)$, where $|ğ¸|$ and $ğ‘‘$ are the numbers of edges and classes, respectively, and $ğ·$ is the dimension of the node feature. The complexity of the representation learner is $ğ‘‚(ğ‘)$. Therefore, the complexity of our model is $ğ‘‚(ğ¾(ğ‘ğ‘ƒ+ğœ‚(|ğ¸|ğ·ğ‘‘+ğ‘)))$, where $ğ¾$ is the number of source domains and $ğ¾,ğ‘ƒ,ğœ‚ â‰ª ğ‘$, $ğ·,ğ‘‘â‰ª|ğ¸|$.

**[W3]:** Thank for your suggestions. We provide T-sne visualizations demonstrating the effectiveness of the representation learner at [[link](https://anonymous.4open.science/r/MLDGG_pic-DE35/tsne_graph.jpg)]. The domain-invariant semantic factors ğ‘  (middle) and domain-specific variation factors ğ‘£ (right) are disentangled from the node representations ğ‘Ÿ (left) learned from GNNs. We can see that the samples represented by $ğ‘ $ are more distinguished from the ones represented by $ğ‘Ÿ$. The samples represented by $ğ‘£$ are independent of classes. Our T-sne results demonstrate the effectiveness of the representation learner.

**[Q1]:** As the baseline methods are unable to adapt to varying feature and label dimensions, it becomes necessary to adjust them to the same dimension. Among the common approaches for aligning feature dimensions are (1) *torch.nn.ReflectionPad*, (2) *torch.nn.ReplicationPad*, (3) *torch.nn.ZeroPad*, and (4) *torch.nn.ConstantPad*. We evaluated each of these methods and found that zero-padding yielded the best results, thus we adopted it.
