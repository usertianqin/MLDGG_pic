Thank you very much for taking the time to review our paper. Your insights are invaluable in enhancing the quality and clarity of our research. We have carefully considered each of the points you raisedï¼š

**[W1,Q1]:** We assess three fundamental modules  (the structure learner (SL), representation learner (SV), and meta-learner (MAML)) of our proposed model of two variants (MLDGG and MLDGG-ind) on the WebKB dataset by removing each module individually (in section 6.3). Results are given in Fig. 5 (main paper) and Fig. 6 (appendix).

- In *w/o SL*, we observe declines of 2% to 3% in accuracy across all settings compared to the full model. Given that GNNs often aggregate task-irrelevant information, which can result in overfitting and diminish generalization performance, the introduction of the structure learner becomes crucial. By mitigating the adverse effects of task-unrelated edges, the structure learner facilitates the acquisition of comprehensive node representations, thereby improving the overall performance.

- In *w/o SV*, we observe a more substantial loss in performance degradation to 3% to 6% across all settings. This indicates that the disentanglement of semantic and variation factors can enhance the model's capability of generalization. Class labels are dependent on semantic factors, while variation factors representing domain-specific elements are not associated with these labels. When the representation learner is absent (resulting in no disentanglement), performance degradation occurs, particularly in the presence of OOD samples stemming from distributional shifts in target domains. Therefore, mitigating the influence of variation factors becomes crucial for improving the model's robustness across diverse domains.

- In *w/o MAML*, it is evident that without the meta-learner, our model significantly decreases model performance by 8% to 10%. This observation indicates the critical role played by the meta-learner modules in facilitating knowledge transfer from source and target graphs. The meta-learner serves as an integration for both the structure learner and representation learner (see Fig.2), thereby enabling efficient knowledge transfer and facilitating effective adaptation to unseen target domains.  

**[W3]:** The template is the latest version downloaded from the link provided by the KDD official website, where "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY" is given on the top of each page.

**[Q2]:** The representation learner is constructed based on a causal generative model on data variables (r, y) and latent semantic and variation factors (s,v), shown in [[link]](https://anonymous.4open.science/r/MLDGG_pic-DE35/causal_graph.jpg) . The causal generative mechanisms p(r|s,v) and p(y|s) are invariant across domains, and the change of prior p(s, v) is the only source of domain change. Based on this, we propose the likelihood in Eq.8. In the process of solving Eq.8 through Eq.10 and Eq.11, semantic and variation factors are disentangled.

**[Q3]:** We are sorry for the inconvenience caused by the excessive length of the sentence. We rephrase the sentence by improving the readability that "The framework integrates a structural learner and a representation learner within the meta-learning paradigm to facilitate performance for domain generalization on graphs. The structure learner mitigates the adverse effects of task-unrelated edges to facilitate the acquisition of comprehensive node representations. The representation learner by disentangling the semantic and variation factors enhances the model's generalization. Finally, the meta-learner transfers the knowledge learned from the structure and representation learner and facilitates adaptation to unseen target domains".
