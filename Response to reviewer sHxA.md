Thanks for your valuable feedback.

1. Existing studies in graph domain generalization [16], [36], [42] solely focus on node classification. We follow their settings and use them as baselines in our experiments. Other tasks on graphs such as link prediction, graph-level classification, etc. require different settings. Your comments have been extremely helpful. We will discuss this in the updated version.

2. We assess three fundamental modules  (the structure learner (SL), representation learner (SV), and meta-learner (MAML)) of our proposed model of two variants (MLDGG and MLDGG-ind) on the WebKB dataset by removing each module individually (in section 6.3). Results are given in Fig. 5 (main paper) and Fig. 6 (appendix).

   - In *w/o SL*, we observe declines of 2% to 3% in accuracy across all settings compared to the full model. Given that GNNs often aggregate task-irrelevant information, which can result in overfitting and diminish generalization performance, the introduction of the structure learner becomes crucial. By mitigating the adverse effects of task-unrelated edges, the structure learner facilitates the acquisition of comprehensive node representations, thereby improving the overall performance.

   - In *w/o SV*, we observe a more substantial loss in performance degradation to 3% to 6% across all settings. This indicates that the disentanglement of semantic and variation factors can enhance the model's capability of generalization. Class labels are dependent on semantic factors, while variation factors representing domain-specific elements are not associated with these labels. When the representation learner is absent (resulting in no disentanglement), performance degradation occurs, particularly in the presence of OOD samples stemming from distributional shifts in target domains. Therefore, mitigating the influence of variation factors becomes crucial for improving the model's robustness across diverse domains.

   - In *w/o MAML*, it is evident that without the meta-learner, our model significantly decreases model performance by 8% to 10%. This observation indicates the critical role played by the meta-learner modules in facilitating knowledge transfer from source and target graphs. The meta-learner serves as an integration for both the structure learner and representation learner (see Fig.2), thereby enabling efficient knowledge transfer and facilitating effective adaptation to unseen target domains. 
